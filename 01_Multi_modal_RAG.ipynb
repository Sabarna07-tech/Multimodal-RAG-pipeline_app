{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal RAG with Amazon Bedrock, Amazon Nova and LangChain\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to implement a multi-modal Retrieval-Augmented Generation (RAG) system using **Amazon Bedrock with Amazon Nova and LangChain**. Many documents contain a mixture of content types, including text and images. Traditional RAG applications often lose valuable information captured in images. With the emergence of Multimodal Large Language Models (MLLMs), we can now leverage both text and image data in our RAG systems.\n",
    "\n",
    "In this notebook, we'll explore one approach to multi-modal RAG (`Option 1`):\n",
    "\n",
    "1. Use multimodal embeddings (such as Amazon Titan) to embed both images and text\n",
    "2. Retrieve relevant information using similarity search\n",
    "3. Pass raw images and text chunks to a multimodal LLM for answer synthesis using Amazon Nova\n",
    "\n",
    "We'll use the following tools and technologies:\n",
    "\n",
    "- [LangChain](https://python.langchain.com/v0.2/docs/introduction/) to build a multimodal RAG system\n",
    "- [faiss](https://github.com/facebookresearch/faiss) for similarity search\n",
    "- [Amazon Nova](https://docs.aws.amazon.com/nova/latest/userguide/what-is-nova.html ) for answer synthesis\n",
    "- [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) for image embeddings\n",
    "- [Amazon Bedrock](https://aws.amazon.com/bedrock/) for accessing powerful AI models, like the ones above\n",
    "- [pymupdf](https://pymupdf.readthedocs.io/en/latest/) to parse images, text, and tables from documents (PDFs)\n",
    "- [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) for interacting with Amazon Bedrock\n",
    "\n",
    "This approach allows us to create a more comprehensive RAG system that can understand and utilize both textual and visual information from our documents.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have the following packages and dependencies installed:\n",
    "\n",
    "- Python 3.10 or later\n",
    "- langchain\n",
    "- boto3\n",
    "- faiss\n",
    "- pymupdf\n",
    "- tabula\n",
    "- tesseract\n",
    "- requests\n",
    "\n",
    "Let's get started with building our multi-modal RAG system using Amazon Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Multimodal RAG with Amazon Bedrock](imgs/multimodal-rag1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Importing the libs\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade jpype1 tabula-py PyMuPDF\n",
    "# !pip install --upgrade boto3 requests numpy tqdm botocore langchain ipython\n",
    "# !pip install --upgrade faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tabula\n",
    "import faiss\n",
    "import json\n",
    "import base64\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from botocore.exceptions import ClientError\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from IPython import display\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import numpy as np\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Loading\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf  # ensure you have PyMuPDF installed\n",
    "import tabula\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: E:\\rag_edubot\\data\\attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# Downloading the dataset - URL of the \"Attention Is All You Need\" paper (Replace it with the URL of the PDF file/dataset you want to download)\n",
    "# Download and display a sample PDF\n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "filename = \"attention_paper.pdf\"\n",
    "filepath = r\"E:\\rag_edubot\\data\\attention_paper.pdf\"\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filepath, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully: {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"E:\\rag_edubot\\data\\attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x114fe221040>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open and display the PDF\n",
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"600\"\n",
       "            src=\"E:\\rag_edubot\\data\\attention_paper.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x114fe1f9940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the PDF file\n",
    "display.IFrame(filepath, width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "   Data Extraction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create directories for outputs\n",
    "def create_directories(base_dir):\n",
    "    os.makedirs(f\"{base_dir}/text\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/tables\", exist_ok=True)\n",
    "    os.makedirs(f\"{base_dir}/page_images\", exist_ok=True)\n",
    "base_dir = \"data\"\n",
    "create_directories(base_dir)\n",
    "\n",
    "# Process tables, text, and images (your existing functions)\n",
    "def process_tables(doc, page_num, base_dir, items):\n",
    "    try:\n",
    "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
    "        for i, table in enumerate(tables):\n",
    "            table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{i}.csv\"\n",
    "            table.to_csv(table_file_name, index=False)\n",
    "            # Save table text as a string (for embedding)\n",
    "            table_text = table.to_csv(index=False)\n",
    "            items.append({\"page\": page_num, \"type\": \"table\", \"text\": table_text, \"path\": table_file_name})\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting tables from page {page_num}: {e}\")\n",
    "\n",
    "\n",
    "# Process text chunks\n",
    "def process_text_chunks(text, text_splitter, page_num, base_dir, items):\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        text_file_name = f\"{base_dir}/text/{os.path.basename(filepath)}_text_{page_num}_{i}.txt\"\n",
    "        with open(text_file_name, 'w', encoding='utf-8') as f:\n",
    "            f.write(chunk)\n",
    "        items.append({\"page\": page_num, \"type\": \"text\", \"text\": chunk, \"path\": text_file_name})\n",
    "\n",
    "# Process images\n",
    "def process_images(page, page_num, base_dir, items):\n",
    "    images = page.get_images()\n",
    "    for idx, image in enumerate(images):\n",
    "        xref = image[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref)\n",
    "        image_name = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
    "        pix.save(image_name)\n",
    "        with open(image_name, 'rb') as f:\n",
    "            encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
    "        items.append({\"page\": page_num, \"type\": \"image\", \"path\": image_name, \"image\": encoded_image})\n",
    "# Process page images\n",
    "def process_page_images(page, page_num, base_dir, items):\n",
    "    pix = page.get_pixmap()\n",
    "    page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
    "    pix.save(page_path)\n",
    "    with open(page_path, 'rb') as f:\n",
    "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
    "    items.append({\"page\": page_num, \"type\": \"page\", \"path\": page_path, \"image\": page_image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 0: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 1: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 2: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  33%|███▎      | 5/15 [00:00<00:00, 16.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 3: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 4: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 5: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 6: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 7: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 8: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  80%|████████  | 12/15 [00:00<00:00, 25.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 9: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 10: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 11: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 12: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 13: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 15/15 [00:00<00:00, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 14: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "base_dir = \"data\"\n",
    "print(f\"Number of pages in the PDF: {num_pages}\")\n",
    "# Creating the directories\n",
    "create_directories(base_dir)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, length_function=len)\n",
    "items = []\n",
    "\n",
    "# Process each page of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    process_tables(doc, page_num, base_dir, items)\n",
    "    process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
    "    process_images(page, page_num, base_dir, items)\n",
    "    process_page_images(page, page_num, base_dir, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, length_function=len)\n",
    "items = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  13%|█▎        | 2/15 [00:00<00:00, 18.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 0: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 1: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 2: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  53%|█████▎    | 8/15 [00:00<00:00, 20.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 3: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 4: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 5: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 6: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 7: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages:  80%|████████  | 12/15 [00:00<00:00, 24.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 8: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 9: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 10: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 11: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 12: module 'tabula' has no attribute 'read_pdf'\n",
      "Error extracting tables from page 13: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDF pages: 100%|██████████| 15/15 [00:00<00:00, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting tables from page 14: module 'tabula' has no attribute 'read_pdf'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process each page of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    text = page.get_text()\n",
    "    process_tables(doc, page_num, base_dir, items)\n",
    "    process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
    "    process_images(page, page_num, base_dir, items)\n",
    "    process_page_images(page, page_num, base_dir, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text Embeddings using Sentence Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def generate_text_embedding(text):\n",
    "    embedding = text_model.encode(text)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Image Embeddings using CLIP from Hugging Face\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_embedding(image_base64):\n",
    "    # Decode base64 image\n",
    "    image_data = base64.b64decode(image_base64)\n",
    "    image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
    "    # Process the image for CLIP\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    # Normalize the embedding vector\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    return image_features.squeeze(0).cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding vector dimension (for consistency, e.g., 384 for text and 512 for CLIP)\n",
    "# Note: The dimensions might differ between models. You might consider mapping to a unified dimension if needed.\n",
    "text_embedding_dimension = 384  # all-MiniLM-L6-v2 outputs 384-dim vectors\n",
    "image_embedding_dimension = 512  # CLIP-vit-base-patch32 outputs 512-dim vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 101/101 [00:03<00:00, 27.89it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(items), desc=\"Generating embeddings\") as pbar:\n",
    "    for item in items:\n",
    "        if item['type'] in ['text', 'table']:\n",
    "            # Use text embedding model\n",
    "            item['embedding'] = generate_text_embedding(item['text'])\n",
    "        elif item['type'] in ['image', 'page']:\n",
    "            # Use image embedding model\n",
    "            item['embedding'] = generate_image_embedding(item['image'])\n",
    "        else:\n",
    "            item['embedding'] = None  # in case of unknown type\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 83 embeddings.\n"
     ]
    }
   ],
   "source": [
    "# Optionally, create a unified FAISS index for one modality or maintain separate indexes.\n",
    "# For example, here we'll create a FAISS index for text embeddings.\n",
    "import faiss\n",
    "\n",
    "# Collect only text/table embeddings (or choose modality as needed)\n",
    "text_embeddings = [np.array(item['embedding'], dtype=np.float32) \n",
    "                   for item in items if item['type'] in ['text', 'table'] and item['embedding'] is not None]\n",
    "\n",
    "if len(text_embeddings) > 0:\n",
    "    # Create a FAISS index (using L2 distance)\n",
    "    index = faiss.IndexFlatL2(text_embedding_dimension)\n",
    "    index.reset()\n",
    "    index.add(np.vstack(text_embeddings))\n",
    "    print(f\"FAISS index created with {index.ntotal} embeddings.\")\n",
    "else:\n",
    "    print(\"No text embeddings available to index.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can perform retrieval using the appropriate modality.\n",
    "# For example, to search using a text query:\n",
    "query = \"Which optimizer was used when training the models?\"\n",
    "query_embedding = np.array(generate_text_embedding(query), dtype=np.float32).reshape(1, -1)\n",
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbors indices: [43 44 47 45 25]\n"
     ]
    }
   ],
   "source": [
    "distances, indices = index.search(query_embedding, k=k)\n",
    "print(\"Nearest neighbors indices:\", indices.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded successfully: E:\\rag_edubot\\data\\attention_paper.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "filepath = r\"E:\\rag_edubot\\data\\attention_paper.pdf\"\n",
    "os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "def download_file(url, filepath, max_retries=3):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=(10, 60)) as response:\n",
    "                response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code.\n",
    "                with open(filepath, 'wb') as f:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:  # filter out keep-alive new chunks\n",
    "                            f.write(chunk)\n",
    "            print(f\"File downloaded successfully: {filepath}\")\n",
    "            return\n",
    "        except requests.exceptions.ChunkedEncodingError as e:\n",
    "            print(f\"ChunkedEncodingError encountered: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(f\"ReadTimeout encountered: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e}. Retrying ({retries + 1}/{max_retries})...\")\n",
    "        retries += 1\n",
    "    print(\"Failed to download the file after several retries.\")\n",
    "\n",
    "download_file(url, filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " {'error': '401 Unauthorized'}\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceApi\n",
    "\n",
    "# Initialize the inference client with your API token\n",
    "inference = InferenceApi(repo_id=\"EleutherAI/gpt-j-6B\", token=\"YOUR_API_TOKEN_HERE\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant for question answering.\n",
    "The following context is retrieved from a set of documents:\n",
    "The optimizer used was AdamW.\n",
    "Results show that AdamW outperforms SGD in our experiments.\n",
    "[IMAGE CONTENT]\n",
    "Based on the above context, answer the question: Which optimizer was used when training the models?\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# Create a payload with the prompt and parameters\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 20\n",
    "    }\n",
    "}\n",
    "\n",
    "# Invoke the model using the payload\n",
    "response = inference(payload)\n",
    "print(\"Generated Response:\\n\", response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Open-Source RAG Response Generation\n",
    "# -----------------------------------\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Initialize a text-generation model from Hugging Face.\n",
    "# Here, we use GPT-J-6B as an example; you can substitute another model if desired.\n",
    "# Make sure you have the required model files and a GPU (or set device=-1 for CPU, which is slow).\n",
    "def generate_response_hf(prompt, matched_items):\n",
    "    # Prepare context by concatenating retrieved text\n",
    "    context_parts = []\n",
    "    for item in matched_items:\n",
    "        if item['type'] in ['text', 'table']:\n",
    "            context_parts.append(item.get(\"text\", \"\"))\n",
    "        elif item['type'] in ['image', 'page']:\n",
    "            context_parts.append(\"[IMAGE CONTENT]\")\n",
    "    context = \"\\n\".join(context_parts)\n",
    "    \n",
    "    full_prompt = f\"\"\"You are a helpful assistant for question answering.\n",
    "The following context is retrieved from a set of documents:\n",
    "{context}\n",
    "\n",
    "Based on the above context, answer the question: {prompt}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call Hugging Face Inference API\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/EleutherAI/gpt-j-6B\"\n",
    "    headers = {\"Authorization\": \"Bearer YOUR_API_TOKEN_HERE\"}\n",
    "    response = requests.post(API_URL, headers=headers, json={\n",
    "        \"inputs\": full_prompt,\n",
    "        \"parameters\": {\"max_new_tokens\": 300, \"do_sample\": True, \"top_p\": 0.9, \"top_k\": 20}\n",
    "    })\n",
    "    result = response.json()\n",
    "    # Depending on the API response structure, extract the generated text\n",
    "    generated_text = result[0].get('generated_text', '') if isinstance(result, list) else result.get('generated_text', '')\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which optimizer was used when training the models?\"\n",
    "matched_items = [\n",
    "    {\"type\": \"text\", \"text\": \"The optimizer used was AdamW.\"},\n",
    "    {\"type\": \"table\", \"text\": \"Results show that AdamW outperforms SGD in our experiments.\"},\n",
    "    {\"type\": \"image\", \"image\": \"[Base64ImageData]\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "response_text = generate_response_hf(query, matched_items)\n",
    "print(\"Generated Response:\\n\", response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Creating Vector Database/Index\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# All the embeddings\n",
    "all_embeddings = np.array([item['embedding'] for item in items])\n",
    "\n",
    "# Create FAISS Index\n",
    "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
    "\n",
    "# Clear any pre-existing index\n",
    "index.reset()\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(np.array(all_embeddings, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Generating RAG response with Amazon Nova\n",
    "def invoke_nova_multimodal(prompt, matched_items):\n",
    "    \"\"\"\n",
    "    Invoke the Amazon Nova model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Define your system prompt(s).\n",
    "    system_msg = [\n",
    "                        { \"text\": \"\"\"You are a helpful assistant for question answering. \n",
    "                                    The text context is relevant information retrieved. \n",
    "                                    The provided image(s) are relevant information retrieved.\"\"\"}\n",
    "                 ]\n",
    "\n",
    "    # Define one or more messages using the \"user\" and \"assistant\" roles.\n",
    "    message_content = []\n",
    "\n",
    "    for item in matched_items:\n",
    "        if item['type'] == 'text' or item['type'] == 'table':\n",
    "            message_content.append({\"text\": item['text']})\n",
    "        else:\n",
    "            message_content.append({\"image\": {\n",
    "                                                \"format\": \"png\",\n",
    "                                                \"source\": {\"bytes\": item['image']},\n",
    "                                            }\n",
    "                                    })\n",
    "\n",
    "\n",
    "    # Configure the inference parameters.\n",
    "    inf_params = {\"max_new_tokens\": 300, \n",
    "                \"top_p\": 0.9, \n",
    "                \"top_k\": 20}\n",
    "\n",
    "    # Define the final message list\n",
    "    message_list = [\n",
    "        {\"role\": \"user\", \"content\": message_content}\n",
    "    ]\n",
    "    \n",
    "    # Adding the prompt to the message list\n",
    "    message_list.append({\"role\": \"user\", \"content\": [{\"text\": prompt}]})\n",
    "\n",
    "    native_request = {\n",
    "        \"messages\": message_list,\n",
    "        \"system\": system_msg,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "\n",
    "    # Initialize the Amazon Bedrock runtime client\n",
    "    model_id = \"amazon.nova-pro-v1:0\"\n",
    "    client = ChatBedrock(model_id=model_id)\n",
    "\n",
    "    # Invoke the model and extract the response body.\n",
    "    response = client.invoke(json.dumps(native_request))\n",
    "    model_response = response.content\n",
    "    \n",
    "    return model_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Query\n",
    "query = \"Which optimizer was used when training the models?\"\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Check the result (matched chunks)\n",
    "result.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the matched items\n",
    "matched_items = [{k: v for k, v in items[index].items() if k != 'embedding'} for index in result.flatten()]\n",
    "\n",
    "# Generate RAG response with Amazon Nova\n",
    "response = invoke_nova_multimodal(query, matched_items)\n",
    "\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Your Turn: Test the RAG Pipeline\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# List of queries (Replace with any query of your choice)\n",
    "other_queries = [\"How long were the base and big models trained?\",\n",
    "                 \"Which optimizer was used when training the models?\",\n",
    "                 \"What is the position-wise feed-forward neural network mentioned in the paper?\",\n",
    "                 \"What is the BLEU score of the model in English to German translation (EN-DE)?\",\n",
    "                 \"How is the scaled-dot-product attention is calculated?\",\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "query = other_queries[0] # Replace with any query from the list above\n",
    "\n",
    "# Generate embeddings for the query\n",
    "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
    "\n",
    "# Search for the nearest neighbors in the vector database\n",
    "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)\n",
    "\n",
    "# Retrieve the matched items\n",
    "matched_items = [{k: v for k, v in items[index].items() if k != 'embedding'} for index in result.flatten()]\n",
    "\n",
    "# Generate RAG response with Amazon Nova\n",
    "response = invoke_nova_multimodal(query, matched_items)\n",
    "\n",
    "# Display the response\n",
    "display.Markdown(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4, #1e90ff); \n",
    "            color: white; \n",
    "            padding: 15px; \n",
    "            border-radius: 10px; \n",
    "            text-align: center; \n",
    "            font-family: 'Comic Sans MS', cursive, sans-serif; \n",
    "            text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Thank you!\n",
    "</h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
